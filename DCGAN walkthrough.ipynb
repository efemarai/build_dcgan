{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joined-distributor",
   "metadata": {},
   "source": [
    "# Building Complex Computational Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "final-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as tf\n",
    "\n",
    "\n",
    "import efemarai as ef\n",
    "ef.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-breast",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-mexico",
   "metadata": {},
   "source": [
    "Generator takes in a random vector and should generate an image. For this we will use [transposed convolutions](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-scott",
   "metadata": {},
   "source": [
    "![Transposed Convolution](https://miro.medium.com/max/700/1*faRskFzI7GtvNCLNeCN8cg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-territory",
   "metadata": {},
   "source": [
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "naval-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0070,  0.1147, -0.0082,  0.0899],\n",
      "          [-0.0530, -0.0626,  0.0393,  0.1251],\n",
      "          [ 0.1381,  0.0016, -0.0353, -0.0460],\n",
      "          [-0.0163,  0.0628, -0.1509, -0.1345]],\n",
      "\n",
      "         [[-0.0332, -0.0538, -0.0272, -0.0673],\n",
      "          [ 0.0603,  0.0927,  0.0434, -0.0034],\n",
      "          [-0.0275,  0.1481, -0.0456,  0.1091],\n",
      "          [-0.0347, -0.0176,  0.0646, -0.0299]],\n",
      "\n",
      "         [[-0.0167,  0.0346,  0.0582, -0.0888],\n",
      "          [-0.0999,  0.0241,  0.0198,  0.0375],\n",
      "          [ 0.0987,  0.0226, -0.0047,  0.0071],\n",
      "          [-0.0548,  0.0366,  0.0696,  0.0116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1380, -0.1157,  0.0782,  0.0628],\n",
      "          [-0.0769, -0.0766, -0.0798,  0.0277],\n",
      "          [ 0.0638, -0.1095,  0.0535, -0.1067],\n",
      "          [-0.0415, -0.0228, -0.0058, -0.0490]],\n",
      "\n",
      "         [[-0.0453,  0.0977,  0.0230,  0.1073],\n",
      "          [-0.0495,  0.0333, -0.0700,  0.0720],\n",
      "          [-0.0706, -0.0074, -0.0627, -0.0353],\n",
      "          [-0.0118,  0.0436, -0.0879,  0.0081]],\n",
      "\n",
      "         [[ 0.0773,  0.0827, -0.1172,  0.0480],\n",
      "          [-0.0655, -0.0077,  0.0149, -0.0682],\n",
      "          [ 0.0743, -0.0376,  0.1387, -0.0663],\n",
      "          [-0.0885,  0.0640,  0.0728, -0.0883]]]],\n",
      "       grad_fn=<SlowConvTranspose2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "nz = 16\n",
    "noise = torch.randn(1, nz, 1, 1)\n",
    "\n",
    "convT = nn.ConvTranspose2d(\n",
    "    in_channels=nz, \n",
    "    out_channels=64, \n",
    "    kernel_size=4, \n",
    "    stride=1, \n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "print(convT(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-rover",
   "metadata": {},
   "source": [
    "When working with tensors printing tensors rarely gives you useful information. How about something more visual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "direct-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef.print(convT(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-dependence",
   "metadata": {},
   "source": [
    "Using Efemarai's `print()` function automatically generates a 3D visualizaton of the tensors where you can easily inspect any element or check out the values distribution with a few mouse clicks. Up to 6D tensors are supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-california",
   "metadata": {},
   "source": [
    "![Print Tensor](imgs/ef_print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-death",
   "metadata": {},
   "source": [
    "Seeing the resulting tensor from a computation is useful, but what's even more useful is to explore the computation itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "controlled-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ef.scan():\n",
    "    output = convT(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-price",
   "metadata": {},
   "source": [
    "![Graph Scan](imgs/ef_scan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-ceiling",
   "metadata": {},
   "source": [
    "Now lets create our generator module starting with the first layer of transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "noble-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # input is going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-redhead",
   "metadata": {},
   "source": [
    "and explore what's happening with the input noise passing through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "generic-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(nz)\n",
    "with ef.scan(wait=False):\n",
    "    output = gen(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-november",
   "metadata": {},
   "source": [
    "The complete generator contains 5 transposed convolution layers and outputs an image of size `(3, 64, 64)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "known-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # input is going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-combat",
   "metadata": {},
   "source": [
    "Here is what the final computational graph of the generator looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "instant-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "with ef.scan(wait=False):\n",
    "    output = gen(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-trademark",
   "metadata": {},
   "source": [
    "![Generator Graph](imgs/generator_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-particular",
   "metadata": {},
   "source": [
    "You can easily confirm that\n",
    "* all the layers are connected as expected\n",
    "* all computations go as expected - there are no NaNs or Infs\n",
    "* the input vector is correctly transformed into a 3x64x64 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-stadium",
   "metadata": {},
   "source": [
    "### Overfit to a small batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-minority",
   "metadata": {},
   "source": [
    "Load images from the CelebA dataset and create a small batch of `(noise, image)` that we are going to overfit to in order to make sure that our generator can generate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dress-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100\n",
    "\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=\"data\",\n",
    "    transform=tf.Compose([\n",
    "        tf.Resize(64),\n",
    "        tf.CenterCrop(64),\n",
    "        tf.ToTensor(),\n",
    "        tf.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]))\n",
    "\n",
    "class GeneratorMiniBatch(torch.utils.data.Dataset):\n",
    "    def __init__(self, size): \n",
    "        self.size = size\n",
    "        self.noise = torch.randn(size, nz, 1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.noise[idx], dataset[idx][0]\n",
    "\n",
    "generator_minibatch = GeneratorMiniBatch(50)   \n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    generator_minibatch, batch_size=10, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-security",
   "metadata": {},
   "source": [
    "Loop over the minibatch to train and scan the execution periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "devoted-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.465360552072525\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.12741057574748993\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.04939030110836029\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.05862098187208176\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.03839258477091789\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.022419793531298637\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.01586761884391308\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.024093030020594597\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.00936425942927599\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "Tensor of shape (512, 256, 4, 4) cannot be visualized with the current GPU.\n",
      "0.013083366677165031\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    generator.parameters(), lr=1e-3, \n",
    ")\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(100):\n",
    "    for noise, image in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with ef.scan(\n",
    "            iteration, \n",
    "            enabled=iteration % 50 == 0, \n",
    "            wait=False,\n",
    "        ):\n",
    "            output = generator(noise)\n",
    "            loss = (output - image).square().mean()\n",
    "            loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-uruguay",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "indirect-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=False),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=False),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=False),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=False),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert torch.all(0.0 < input) and torch.all(input < 1.0)\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-fourth",
   "metadata": {},
   "source": [
    "### Overfit to a small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "wireless-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorMiniBatch(torch.utils.data.Dataset): \n",
    "    def __init__(self):\n",
    "        self.size = 2 * len(generator_minibatch)\n",
    "        self.fakes = generator(generator_minibatch.noise).detach()\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.fakes):\n",
    "            return self.fakes[idx], torch.zeros(1, dtype=torch.float).squeeze()\n",
    "        else:        \n",
    "            _, image = generator_minibatch[idx - len(self.fakes)]\n",
    "            return image, torch.ones(1, dtype=torch.float).squeeze()\n",
    "\n",
    "\n",
    "discriminator_minibatch = DiscriminatorMiniBatch()   \n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    discriminator_minibatch, batch_size=10, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "common-sauce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-9175a492f485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    discriminator.parameters(), lr=1e-4, \n",
    ")\n",
    "\n",
    "iteration = 0\n",
    "for epoch in range(50):\n",
    "    for image, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = discriminator(image).view(-1)\n",
    "        loss = F.binary_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-allen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
